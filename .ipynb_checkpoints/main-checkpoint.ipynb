{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7bbe20-9976-44db-ad64-d244de77f570",
   "metadata": {},
   "source": [
    "WEB CAM + GRAPHICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d7862ba-7d2e-4cad-897b-255e55888cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\bened\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\bened\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: cvzone in c:\\users\\bened\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\bened\\anaconda3\\lib\\site-packages (from cvzone) (4.9.0.80)\n",
      "Requirement already satisfied: numpy in c:\\users\\bened\\anaconda3\\lib\\site-packages (from cvzone) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install cvzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510659d1-809b-4cf6-b7b1-b8afcfdc4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import cvzone\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import db\n",
    "from firebase_admin import storage\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017edadc-49d7-41eb-92c9-9d8a1dedde35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = credentials.Certificate(\"serviceAccountKey.json\")\n",
    "firebase_admin.initialize_app(cred, {\n",
    "    'databaseURL': 'https://faceattendacerealtime-65507-default-rtdb.firebaseio.com/',\n",
    "    'storageBucket': \"faceattendacerealtime-65507.appspot.com\"\n",
    "})\n",
    "bucket = storage.bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f71d4a2-b18b-4b37-b625-6723d6541399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640)\n",
    "cap.set(4, 480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d71180d-4725-4b57-91ce-b7b62e627905",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgBackground = cv2.imread('Resources/background.png')\n",
    "\n",
    "#importing the mode images into a list\n",
    "folderModePath = 'Resources/Modes'\n",
    "modePathList = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "\n",
    "#print(modePathList)\n",
    "for path in modePathList:\n",
    "    imgModeList.append(cv2.imread(os.path.join(folderModePath,path)))\n",
    "\n",
    "#print(len(imgModeList))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0638c5a6-921c-43a3-93a5-b30409265110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Encode File ...\n",
      "Encode File loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Encode File ...\")\n",
    "file = open('EncodeFile.p','rb')\n",
    "encodeListKnownWithIds = pickle.load(file)\n",
    "file.close()\n",
    "encodeListKnown,studentIds = encodeListKnownWithIds\n",
    "print(\"Encode File loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9306e35-c9f6-4e77-bdbc-583b0bcb4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeType = 0\n",
    "counter = 0\n",
    "id = -1\n",
    "imgStudent = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52ffa3-e03e-4210-aa13-cf0ba13e0501",
   "metadata": {},
   "source": [
    "LOAD THE ENCORDING FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97727fb-76d3-4ee8-aa6f-264528330953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'last_attendance_time': '2022-12-11 00:54:34', 'major': 'CSE', 'name': 'Benedict Paul', 'standing': 'G', 'starting_year': 2021, 'total_attendance': 32, 'total_attendance_time': '2024-04-29 20:58:51', 'year': 4}\n",
      "43704381.508056\n",
      "{'last_attendance_time': '2022-12-11 00:54:34', 'major': 'CSE', 'name': 'Benedict Paul', 'standing': 'G', 'starting_year': 2021, 'total_attendance': 33, 'total_attendance_time': '2024-04-29 21:00:55', 'year': 4}\n",
      "43704390.806631\n",
      "{'last_attendance_time': '2022-12-11 00:54:34', 'major': 'CSE', 'name': 'Benedict Paul', 'standing': 'G', 'starting_year': 2021, 'total_attendance': 34, 'total_attendance_time': '2024-04-29 21:01:05', 'year': 4}\n",
      "43704399.703107\n",
      "{'last_attendance_time': '2022-12-11 00:54:34', 'major': 'CSE', 'name': 'Benedict Paul', 'standing': 'G', 'starting_year': 2021, 'total_attendance': 35, 'total_attendance_time': '2024-04-29 21:01:14', 'year': 4}\n",
      "43704409.753145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m imgS \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(imgS,cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      7\u001b[0m faceCurFrame \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_locations(imgS)\n\u001b[1;32m----> 8\u001b[0m encodeCurFrame \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_encodings(imgS, faceCurFrame)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m faceCurFrame:\n\u001b[0;32m     12\u001b[0m     imgBackground[\u001b[38;5;241m162\u001b[39m:\u001b[38;5;241m162\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m480\u001b[39m, \u001b[38;5;241m55\u001b[39m:\u001b[38;5;241m55\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m640\u001b[39m] \u001b[38;5;241m=\u001b[39m img\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(face_encoder\u001b[38;5;241m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(face_encoder\u001b[38;5;241m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    imgS = cv2.resize(img, (0, 0), None, 0.25, 0.25)\n",
    "    imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurFrame = face_recognition.face_locations(imgS)\n",
    "    encodeCurFrame = face_recognition.face_encodings(imgS, faceCurFrame)\n",
    "\n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "    if faceCurFrame:\n",
    "        for encodeFace, faceLoc in zip(encodeCurFrame, faceCurFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "            # print(\"matches\", matches)\n",
    "            # print(\"faceDis\", faceDis)\n",
    "\n",
    "            matchIndex = np.argmin(faceDis)\n",
    "            # print(\"Match Index\", matchIndex)\n",
    "\n",
    "            if matches[matchIndex]:\n",
    "                # print(\"Known Face Detected\")\n",
    "                # print(studentIds[matchIndex])\n",
    "                y1, x2, y2, x1 = faceLoc\n",
    "                y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "                bbox = 55 + x1, 162 + y1, x2 - x1, y2 - y1\n",
    "                imgBackground = cvzone.cornerRect(imgBackground, bbox, rt=0)\n",
    "                id = studentIds[matchIndex]\n",
    "                if counter == 0:\n",
    "                    cvzone.putTextRect(imgBackground, \"Loading\", (275, 400))\n",
    "                    cv2.imshow(\"Face Attendance\", imgBackground)\n",
    "                    cv2.waitKey(1)\n",
    "                    counter = 1\n",
    "                    modeType = 1\n",
    "\n",
    "        if counter != 0:\n",
    "\n",
    "            if counter == 1:\n",
    "                # Get the Data\n",
    "                studentInfo = db.reference(f'Students/{id}').get()\n",
    "                print(studentInfo)\n",
    "                # Get the Image from the storage\n",
    "                blob = bucket.get_blob(f'Images/{id}.png')\n",
    "                array = np.frombuffer(blob.download_as_string(), np.uint8)\n",
    "                imgStudent = cv2.imdecode(array, cv2.COLOR_BGRA2BGR)\n",
    "                # Update data of attendance\n",
    "                datetimeObject = datetime.strptime(studentInfo['last_attendance_time'],\n",
    "                                                   \"%Y-%m-%d %H:%M:%S\")\n",
    "                secondsElapsed = (datetime.now() - datetimeObject).total_seconds()\n",
    "                print(secondsElapsed)\n",
    "                if secondsElapsed > 30:\n",
    "                    ref = db.reference(f'Students/{id}')\n",
    "                    studentInfo['total_attendance'] += 1\n",
    "                    ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "                    ref.child('last_attendance_time').set(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                else:\n",
    "                    modeType = 3\n",
    "                    counter = 0\n",
    "                    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "            if modeType != 3:\n",
    "\n",
    "                if 10 < counter < 20:\n",
    "                    modeType = 2\n",
    "\n",
    "                imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "                if counter <= 10:\n",
    "                    cv2.putText(imgBackground, str(studentInfo['total_attendance']), (861, 125),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 1)\n",
    "                    cv2.putText(imgBackground, str(studentInfo['major']), (1006, 550),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    cv2.putText(imgBackground, str(id), (1006, 493),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    cv2.putText(imgBackground, str(studentInfo['standing']), (910, 625),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                    cv2.putText(imgBackground, str(studentInfo['year']), (1025, 625),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                    cv2.putText(imgBackground, str(studentInfo['starting_year']), (1125, 625),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "\n",
    "                    (w, h), _ = cv2.getTextSize(studentInfo['name'], cv2.FONT_HERSHEY_COMPLEX, 1, 1)\n",
    "                    offset = (414 - w) // 2\n",
    "                    cv2.putText(imgBackground, str(studentInfo['name']), (808 + offset, 445),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 1, (50, 50, 50), 1)\n",
    "\n",
    "                    imgBackground[175:175 + 216, 909:909 + 216] = imgStudent\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "                if counter >= 20:\n",
    "                    counter = 0\n",
    "                    modeType = 0\n",
    "                    studentInfo = []\n",
    "                    imgStudent = []\n",
    "                    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "    else:\n",
    "        modeType = 0\n",
    "        counter = 0\n",
    "    # cv2.imshow(\"Webcam\", img)\n",
    "    cv2.imshow(\"Face Attendance\", imgBackground)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91889b13-3b42-45ff-8bb8-de170818a89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
